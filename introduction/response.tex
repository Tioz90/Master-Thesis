\section{Response} \label{sec:response}
In order to contextualise the current work, Chapter \ref{chap:literature-review} will investigate the notion of \textit{explainability}, its importance and how to evaluate it, as defined in the current xAI literature.
In particular, the explainability of Bayesian networks will be reviewed in detail as this model will be the basis for the methods carried out in this thesis.

To address the gaps identified in the previous section and in the following chapter, the work conducted in this thesis will concentrate on explainability in the medical domain and will present both a practical and a theoretical part.
The methods will include the implementation of a Bayesian network-based system, inspired by the work by \citet{Butz2018} (see Section \ref{sec:explaining-the-most-probable-explanation}), and its subsequent evaluation.
This recent xAI paper never provided any results for the compelling methods it presented, thus a proof of concept system implementing them, together with novel extensions, will be developed.
The system will be a means of exploring the efficacy of the explanatory modes for BNs - as identified by \citet{lacave2002review} - mainly the \textit{graphical}, \textit{linguistic} and, in particular, the \textit{dialogical}.
Their taxonomy for BN explanations, together with the psychological characteristics of an explanation \citep{miller2018explanation}, will be the framework against which the methods developed in Chapters \ref{chap:methodology} and evaluated in Chapter \ref{chap:results} will be measured.
The explainability of the work will be validated by real medical experts, in a concrete setting, over a period of time; this will also be a means to provide a validation for the methods of the initial paper \citep{Butz2018} and, more in general, of Bayesian networks as a whole.
The system will be a proxy to explore the explainability of BNs that should, in theory, be well adapted to giving highly effective explanations (as discussed in Section \ref{sec:explainability-in-bayesian-networks}).
The hope is also to set a methodological precedent for other \textit{application-grounded evaluations} (see \citep{doshi2017towards} and Section \ref{sec:evaluation-of-explainability}), with the aim of helping to reduce the gap in the xAI literature of the absence of actual human evaluation of explainability.

The methodology that will be used to evaluate these hypotheses will start with the study of a specific, recent xAI paper by \citet{Butz2018}, whose method will become the basis for the work carried out.
Then a proof of concept system will be developed, coded in Python and that will implement the learning and updating of a Bayesian network (see Subsections \ref{subsec:learning-bn-structure}, \ref{subsec:learning-bn-parameters} and \ref{subsec:bnupdating}) together with standard methods (see Subsection \ref{subsec:algorithms}) and novel algorithms (see Subsection \ref{subsec:algorithms-novel}).
The system's main aim will be to \textit{support medical decision making} by establishing a dialogue with the domain expert user and to evaluate the usefulness, in terms of explainability, of various other interaction modes such as conditional probability and most probable explanation queries (see Subsection \ref{subsec:bnupdating}); various algorithms and interaction paradigms will be developed to do so, as detailed in Section \ref{sec:novel-contributions}.
Finally, this system will be evaluated by expert pathologists at \textit{Istituto Cantonale di Patologia} of Locarno (Switzerland), the partner institution who will collaborate on the testing and evaluation of the software tool that will be developed.
The data set that will be used to learn the Bayesian network, composed by the clinical profiles of over 3000 breast cancer patients in the Swiss canton of Ticino, will also be contributed by the institute.
The evaluation will be done through the use of interviews, questionnaires and observation of the experts at work, with the objective of \textit{clinically validating} the system and \textit{assessing its capacity to relate to the medical professionals} in a meaningful manner.

The objective of such a methodology will be to both validate the claims made by \citet{Butz2018} and, more generally, to investigate the assertions made in the literature regarding the explainability of Bayesian networks (see Section \ref{sec:explainability-in-bayesian-networks}); at the same time, the aim is to also identify which characteristics of these models may be the most important in enabling their \textit{comprehensibility}.