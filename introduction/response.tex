\section{Response} \label{sec:response}
In order to contextualise the current work, Chapter \ref{chap:literature-review} will investigate the notions of \textit{explainability}, its importance and how to evaluate it, as defined in the current xAI literature.
In particular, the explainability of Bayesian Networks is reviewed in detail as these will be the basis of the methods carried out in this thesis.

To address the gaps identified in the previous section and in the following chapter, the work carried out in this thesis will concentrate on explainability in the medical domain and will present both a practical part and a theoretical one.
The methods of this thesis will include the implementation of a Bayesian Network-based system, inspired by the work by \citet{Butz2018} (see Section \ref{sec:explaining-the-most-probable-explanation}) and its evaluation.
This paper never provided any results for the compelling method it presents, thus a proof of concept system implementing it together with novel extensions, will be developed.
The system will be a means of exploring the efficacy of the explanatory modes for BNs - as identified by \citet{lacave2002review} - mainly the \textit{graphical}, \textit{verbal} and, in particular, \textit{dialogue}.
Their taxonomy for BN explanations, together with the psychological characteristics  of an explanation \citep{miller2018explanation}, will be the framework against which the methods developed in Chapters \ref{chap:methodology} and evaluated in Chapter \ref{chap:results} will be set.
The explainability of the work will be validated by real medical experts, in a real setting, over a period of time; this will also be a means to provide a validation for the methods of the initial paper \citep{Butz2018} and, more in general, of Bayesian Networks as a whole.
The system will be a proxy to explore the explainability of BNs that should, in theory, be well positioned in being able to give highly effective explanations (as discussed in Section \ref{sec:explainability-in-bayesian-networks}).
The hope is also to set a methodological precedent for other \textit{application-grounded evaluations} (see \citep{doshi2017towards} and Section \ref{sec:evaluation-of-explainability}), with the aim of further reducing the gap of the absence of actual human evaluation of explainability that is present in the xAI literature.

The methodology that will be used to evaluate these hypotheses will start with study of a specific, recent xAI paper by \citet{Butz2018}, whose method will become the basis for the work carried out.
Then a proof of concept system will developed, coded in Python and implementing the learning and updating of a Bayesian Network (see Subsections \ref{subsec:learning-bn-structure}, \ref{subsec:learning-bn-parameters} and \ref{subsec:bnupdating}) together with standard methods (see Subsection \ref{subsec:algorithms}) and novel algorithms (see Subsection \ref{subsec:algorithms-novel}).
The system's main aim will be to support medical decision making through the instauration of a dialogue with the user/domain expert and to evaluate the usefulness, in terms of explainability, of various other interaction modes such as conditional probability and most probable explanation queries (see Subsection \ref{subsec:bnupdating}); various algorithms and interaction paradigms will be developed to do so, as detailed in Section \ref{sec:novel-contributions}.
Finally, this system will be evaluated by expert pathologists at \textit{Istituto Cantonale di Patologia} of Locarno, Ticino, the partner institution which will collaborate on the testing and evaluation of the proof of concept system that will be developed.
The data set that will be used to learn the Bayesian Network, composed by the clinical profiles of over 3000 breast cancer patients in Ticino, will also be contributed by the institute.
The evaluation will be done through the use of interviews, questionnaires and observation of the experts at work, with the objective of clinically validating the system and assess its capacity to relate to the medical professionals in a meaningful manner.

The objective of such a methodology will be to both validate the claims made by \citet{Butz2018} and, more generally, investigate the claims made in the literature regarding the explainability of Bayesian Networks (see Section \ref{sec:explainability-in-bayesian-networks}).