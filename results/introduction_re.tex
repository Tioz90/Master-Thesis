\section{Introduction} \label{sec:introduction-results}
At a high level, the overall objective of this thesis has been to investigate the \textit{explainability} of a medical AI system.
To this end, a proof of concept system based on a Bayesian network model was developed whose methods were inspired by the paper \citep{Butz2018}
The result is a tool geared towards addressing a key gap in existing xAI research: the lack of application-grounded validations of explainability of ML systems, as discussed in Sections \ref{sec:response} and \ref{sec:methodology-introduction}.
These are: to substantiate the claims made in \citep{Butz2018} regarding the explainability of their method, to extend this method to better explore the space of BN explainability and thus investigate the effectiveness of Bayesian networks in providing explanations to expert users.
 An additional objective is also to set a methodological precedent for an \textit{application-grounded evaluation} of an AI system in the medical domain.
The methods, algorithms and tools underlying the developed system were presented in Subsections \ref{subsec:algorithms} and \ref{subsec:algorithms-novel} while the design rationale was explained in Subsection \ref{subsec:interfacing-user}. 

This chapter will present the experimental results obtained through the implementation of the proof of concept system, informing on both the clinical significance of the results (as described in Subsection \ref{subsec:clinical-validation-methodology}) as well as on the tool's capacity to explain the data set and meaningfully interact with the expert medical user (see Subsection \ref{subsec:explainability-validation}).
A successful clinical evaluation of the system, undertaken in collaboration with the medical professionals of the Istituto Cantonale di Patologia (see Section \ref{sec:data-set}), is of paramount importance as this is one of the prerequisites for the effectiveness of the system and the user perception of its trustworthiness.
Intuitively, there is little hope for a meaningful interaction between man and machine if the former distrusts or does not believe in the latter.
The evaluation of the explainability of the system - and, as a consequence, of both Bayesian networks at large and of the method proposed in \citep{Butz2018} - has also been carried out together with the ICP, by using methods akin to those of the social sciences.
This approach was necessitated by to the nature of the research, as an \textit{application-grounded evaluation} (see Section \ref{sec:evaluation-of-explainability}) process involves humans to a high degree.
As identified in Chapter \ref{chap:literature-review}, it is important for the field of explainable AI to borrow methods from outside computer science, statistics and mathematics in order to meaningfully validate the explainability of ML models.

The tool was compared to the concepts identified in \citet{lacave2002review} (see Section \ref{sec:explainability-in-bayesian-networks}), as this review was deemed to be the most complete present in the literature.
These concepts can easily be seen as defining a \textit{taxonomy} for the classification of explainability approaches to Bayesian networks.
As such, the concepts of the taxonomy will be the frame of reference that the software's functionalities will be compared against.
The system was developed in order to explain all three of the elements that the authors identified as needing clarification in a BN: the \textit{evidence}, the \textit{model} and the \textit{reasoning}.
The explanation for the \textit{reasoning} and the \textit{evidence} are at the core of the methods of this thesis, as the \enquote{dialogues} (see Subsection \ref{subsec:algorithms-novel}) are essentially a way of approximating the MPE problem, which the authors identify as the means to explain the \textit{evidence}.
The \textit{model} is explained by means of both \textit{graphical} and \textit{linguistic} descriptions.
The description of the \textit{reasoning} is also achieved by the \enquote{dialogues} and by the \enquote{pseudo-MPE} method of interaction, as their aim is to make the \enquote{line of thought} of the system clear to the user.
Every interaction mode is also compared with the characteristics that \citet{miller2018explanation} identified as inherent to an explanation, from a psychological perspective; that is, explanations should be: \textit{contrastive}, \textit{selected}, \textit{causal} and \textit{social}.

The chapter is organised as follows:
\begin{itemize}
  \item Section \ref{sec:implemented-tool} presents the developed proof of concept system in detail from a user-centric point of view.
	  Each subsection is dedicated to describing a user interaction mode and integrated throughout these are explainability and clinical significance remarks that were observed first-hand or collected informally from the ICP; that is, these results were not collected through the use of the questionnaire described in Subsection \ref{subsec:explainability-validation}.
	  Every user feature is described in terms of the taxonomy presented by \citep{lacave2002review} and the characteristics identified by \citet{miller2018explanation}.
	\item Section \ref{sec:results-validation-results} reports the formal results of the clinical and explainability validation of the developed system, based on the methods presented in Section \ref{sec:validation}.
	\begin{itemize}
		  \item Subsection \ref{subsec:clinical-validation-results} presents the results of the clinical validation of the system by outlining the results to the questions described in Subsection \ref{subsec:clinical-validation-methodology} and by developing a discussion on the significance of the results.
		  \item Subsection \ref{subsec:explainability-validation-results} exhibits and discusses the results of the explainability validation of the developed system by means of analysing the answers given to the questionnaire presented in Subsection \ref{subsec:explainability-validation}.
	\end{itemize}
	\item Section \ref{sec:pseudo-mpe-evaluation} shows the results of the comparison between the \enquote{pseudo-MPE} (see \ref{subsec:algorithms-novel} under the \enquote{pseudo-MPE from Initial Evidence} header) and the true MPE (Definition \ref{def:mpe}) solution, from a quantitative point of view.
	The qualitative evaluation by the medical experts has been included in the questionnaire (see Appendix \ref{app:questionnaire}).
%	\item Section \ref{sec:bn-prediction-evaluation} quantifies the performance of the BN as a classifier against the machine learning methods listed in Subsection \ref{subsec:algorithms-novel} under the \textbf{Other machine learning Methods} header.
	\item Section \ref{sec:issues} discusses the issues that presented themselves during the implementation of the methods of this thesis, together with the solutions and workarounds found to address them.
\end{itemize}