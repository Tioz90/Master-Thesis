\section{Pseudo-MPE Evaluation} \label{sec:pseudo-mpe-evaluation}
This is not a user-facing feature \textit{per se}, but a way of running a test to compare the outputs of the \enquote{pseudo-MPE} algorithm with the exact solution (this is described in detail in Subsection \ref{subsec:algorithms-novel} under the \enquote{MPE Algorithms Comparison} header).

The Hamming and Jaccard distances between the MPE calculated with Pgmpy's \texttt{map\_query} and with DAOOPT should have been zero, as both use exact methods to solve the MPE problem.
It was seen that this was not the case and this lead to the discovery of the problems described in Subsection \ref{subsec:results-mpe-calculation-issues}.
As is explained in Section \ref{sec:issues}, the benchmark against which to compare the \enquote{pseudo-MPE} was thus chosen to be Pgmpy's \texttt{map\_query} function.

The tests were run over 1000 iterations; that is, 1000 initial random evidence $\boldsymbol{U_e}$ were generated and the \enquote{pseudo-MPE} solution was found using Algorithm \ref{alg:pseudo-mpe-initial-evidence} and was compared with the true MPE solution.
Given that there are twelve variables in the BN, the maximum distance possible for both Hamming and Jaccard distances is 12.
Thus, the average distances shown in Table \ref{tab:mpe-vs-pseudo-results}, denote a good overlap between the ground-truth most probable explanation and the approximate configuration obtained with the \enquote{pseudo-MPE} algorithm.

\begin{table}[h]
	\centering
	\caption{Distance of \enquote{pseudo-MPE} from true MPE solution}
	\begin{tabularx}{0.5\textwidth}{Xr}
		\toprule
		Distance measure & Average distance  \\
		\midrule	
		Hamming & 0.062 \\
		Jaccard & 0.048 \\
		\bottomrule
		\end{tabularx}
	\label{tab:mpe-vs-pseudo-results}
\end{table}

As discussed by \citet{gamez2013advances}, the MPE problem is \textit{NP-hard} \citep{kwisthout2011most} and remains such even in restricted network topologies, networks with restricted probabilities and for constant-bound approximations.
MAP, as anticipated in Subsection \ref{subsec:bnupdating} is a harder problem than MPE and is in fact NP\textsuperscript{PP}-complete and, unlike MPE, remains NP-complete when restricted to polytrees.
The proposed \enquote{pseudo-MPE} algorithm, whose pseudocode is shown in Algorithm \ref{alg:pseudo-mpe-initial-evidence}, reduces the MAP problem to the updating problem.
Updating involves a polynomial number (namely quadratic with respect to the number of variables) of calls of standard updating in a BN and thus has a complexity $O(kV^2)$, in the number of nodes $V$ with $k$ a parameter depending on the algorithm used for the calculation of posterior probabilities.

In order to achieve the explanation of a node, we compute the updated probabilities for all the nodes, at every step the number of available \textit{(state,value)} pairs decreases by $1$ so one fewer updating call is done and the resulting complexity is quadratic $O(V^2)$.
It is simple to see that there are at most only $V$ \textit{(state,value)} pairs because every \textit{state} is paired at each step with only one of its \textit{values} - the most probable.
The single most efficient pair is chosen at each step and then the corresponding \textit{state} is no longer selectable.

The extra term $k$ depends on the algorithm used to compute the posterior probabilities, as at each step these are recalculated based on the updated evidence.
The Pomegranate-based implementation does this inference utilising the \texttt{predict\_proba} built-in function, based on \textit{loopy belief propagation}.
Loopy belief propagation is an \textit{approximate inference algorithm} that estimates a solution to the exact belief propagation algorithm in \textit{quadratic time}; it was introduced by \citet{pearl1982reverend}.