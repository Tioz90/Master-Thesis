\section{Summary} \label{sec:literature-review-summary}
The findings outlined in this chapter refer to the concept of explainability of Machine Learning models, to its importance, to ways of evaluating it and to explainability in the specific case of Bayesian Networks.
The concept of explainability is central to the field of Explainable AI (xAI), whose goal is to make Machine Learning systems \enquote{interpretable}.
Explainability can briefly be defined as the property of a system that is able to \enquote{explain or to present in understandable terms to a human} \citep{dosilovic2018} its outputs.
Two main classes of explainable models have been identified by \citet{mittelstadt2019explaining}: ante-hoc or transparent and post-hoc interpretable ones; the formers are inherently inspectable in their inner workings while the latters are made understandable by way of extra techniques.
These two classes have also been refined by \citet{doshi2017towards} into a four-tier taxonomy consisting of: \textit{opaque}, \textit{interpretable}, \textit{comprehensible} and \textit{explainable systems}.
An opaque system, also know as a \enquote{black-box model}, is one whose inner workings are not inspectable from the outside; an interpretable system corresponds to an ante-hoc interpretable one; a comprehensible one emits extra information together with its output; an explainable system explicitly outputs a human-understandable line of reasoning aimed at clarifying its workings.

Explainability has become a central concept to the field of AI as a whole; as ML models take over more and more functions in our societies, the pressure for them to be able to explain their decisions is increased accordingly.
The \textit{General Data Protection Regulation} (GDPR) that became effective in 2018 was viewed by many as increasing the societal pressure to make systems explainable; many may have been mistaken as regards the actual rules mandated by the regulation - that aren't really prescribing a broad \enquote{right to an explanation} \citep{edwards2018enslaving} - but nonetheless the feeling of urgency is sure to increase the focus of both researchers and laypeople.
In general, explainability is framed as an issue of moral necessity as it easy to find a long series of situations where ML models displayed covert bias or what we would regard as bad moral judgement.

There are all manner of ways to measure explainability and these can be classified into a three-layer taxonomy \citep{doshi2017towards} based on the assumption that the best type of evaluation is the one that most involves humans.
The three classes are \textit{functionally-grounded evaluation}, \textit{human-grounded evaluation} and \textit{application-grounded evaluation}, ordered from the one least involving real humans to the one where the presence of the human-in-the-loop is greatest.
As the involvement of humans in evaluating models' explanations increases, so does the cost of such an experiment and its specificity, as the highest evaluation level necessarily entails the collaboration of domain-experts on specific tasks.
A parallel taxonomy identifies: methods to explain black box models, methods to explain black box outcomes, methods to inspect black boxes and methods to design transparent boxes.
An overarching notion that has been stressed is that \textit{explainability is a graded notion} that depends on the knowledge and expertise of the particular user: different users, with different expertises and backgrounds, may rate the quality of a same explanation very differently.

Bayesian Networks (BNs) have enjoyed widespread appeal in mission-critical domains like that of medicine and thus the drive to develop methods to explain their outputs has always been strong.
BNs have three main elements that necessitate an explanation \citep{lacave2002review}: the \textit{knowledge base}, the \textit{reasoning process} and the \textit{evidence propagated}.
Explaining the first \enquote{consists of determining which values of the unobserved variables justify the available evidence} and is done by solving the Most Probable Explanation (MPE) problem.
For the second, a static explanation of the BN is achieved by displaying it graphically or verbally.
The last element is explained by showing the reasoning that brought the BN to give the outputs it did that can be achieved by providing a justification for its outputs, for the results it did not give or via hypothetical reasoning.
The fact that BNs are able to naturally support counterfactual reasoning, combine single variables into composite outputs and model causality puts them at an advantage compared to other ML systems when generating an effective explanation for a user.
This is because the capabilities of a BN enable it to generate explanations that are uniquely suited to out psychological biases and expectations of what an explanation should entail.

Some of the main gaps that were found during the review of the literature relate to how there is still a great confusion in the field of xAI regarding what an explanation really is and thus what constitutes a good instance of it.
There is also a prevalent methodological confusion, as different authors use terms in incongruous ways, for example sometimes \textit{intepretation} is taken to mean \textit{explanation} while in other cases they refer to different concepts, for example in the taxonomy of interpretable systems proposed by \citet{doshi2017towards}.
This naturally makes it difficult for the field to converge onto methods to evaluate such explanations and this is reflected in the barrage of methods present in the literature, each one focused only on a particular system or instance of model.
This confusion is exacerbated by a seeming lack of interest or awareness of xAI researchers for the sizeable corpus of work in psychology, philosophy, social sciences, neuroscience and human computer interaction that has already investigated the nature of explanations, what desiderata they may possess and which are most effective.
In fact, the great majority of proposed approaches is only focused on proving formal explainability and neglects the human side that is naturally present in any explanation; there has been little work carried out to validate approaches in real settings with real domain experts so many explainability methods are substantiated only at theoretical level.
The underlying issue that has been seen to run transversely across the various concepts investigated in this chapter is best summarised by the idea that \enquote{inmates are running the asylum}, meaning that individual researchers are claiming that their models are interpretable referring only to their own personal views and biases and not to established literature and methods.
It would be hard for them to do otherwise, as the field of xAI seems, at present, to be a collection of diverging strands without a comprehensive program able to help it converge onto its stated goal: to make Machine Learning systems understandable by their users and thus increase their social utility and acceptance.
