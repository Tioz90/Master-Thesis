\documentclass[mscthesis]{usiinfthesis}
\usepackage{lipsum}


\usepackage{listings}

\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}



\title{My Dissertation - A very long title\\ which runs over two
  lines} %compulsory
\specialization{Artificial Intelligence}%optional
\subtitle{Subtitle: Reinventing the World} %optional 
\author{Thomas Francesco Tiotto} %compulsory
\begin{committee}
\advisor{Prof.}{Alessandro Facchini}{} %compulsory
\coadvisor{Prof.}{Alessandro Antonucci}{}{} %optional
\end{committee}
\Day{Yesterday} %compulsory
\Month{September} %compulsory
\Year{2019} %compulsory, put only the year
\place{Lugano} %compulsory

\dedication{To my beloved} %optional
\openepigraph{Someone said \dots}{Someone} %optional

%\makeindex %optional, also comment out \theindex at the end

\begin{document}

\maketitle %generates the titlepage, this is FIXED

\frontmatter %generates the frontmatter, this is FIXED

\begin{abstract}
This is a very abstract abstract. 

\lipsum
\end{abstract}

\begin{acknowledgements}
\lipsum 
\end{acknowledgements}

\tableofcontents 
\listoffigures %optional
\listoftables %optional

\mainmatter

\chapter{Introduction}
\section{Context}
\textbf{state the general topic and give background of what your reader needs to know to understand the problem outline the current situation evaluate the current situation (advantages/ disadvantages) and identify the gap}

While neural networks and the field of Artificial Intelligence (AI) - as a field - have existed for nearly seventy years, the concept of artificial intelligence dates back to at least Ancient Greece.  In ancient times, artificial intelligence was part of the domain of myth; in the twentieth century, of that of science.  During this last decade, artificial intelligence can't anymore fall under any umbrella, as it has materialised out of Man's imagination, broken out of laboratories and is now free in the world at large.

No sector of our economy has been left untouched by the recent and rapid rise of machine learning that has been fuelled by deep neural networks, the availability of Big Data and cheap computing power.  Fields as diverse and critical as government, medicine, finance and bioinformatics have been revolutionised and the possibility has been set for new ones - such as self-driving vehicles - to be born.  
The ever increasing reliance of our society on ever more complex machine learning-driven algorithms can only make us worry ever more about the ethical dilemmas posed by such a situation.  
As more and more decisions are made in an automated way, with many of them significantly impacting both individuals and society at large, it comes natural to stop and wonder what are the characteristics we want the systems making these decisions to have.  

Explainable AI (xAI) is the sub-field of AI that rests at the intersection between Computer Science, Social Sciences and Philosophy, whose aim is to define our desiderata of artificially intelligent systems and machine learning algorithms from the point of view of their explainability.  The basic idea is that the prerequisite for the evaluation of the ethical and moral implications of a machine's decision, is for the system to be ``interpretable'' or ``explainable''.  

Within the xAI community, there is currently no unanimously agreed upon definition of which these desiderata should be or of the best way to implement them in real systems.
%TODO investigare se e' vero che manca definizione comune 
There is also no common, agreed-upon, definition of what is meant by the phrase ``understanding a system'': some authors equate this to having a \textit{functional understanding}, void of the low-level details, while others decline it into the concepts of \textit{interpretation} and \textit{explanation}, the former indicating the output of a format that a human user can comprehend and the latter a set of features that have contributed to generating the system's decision. 

The difficulties start in trying to define what interpretability \textit{even means}.  Does it mean to gain the trust the system's user?  Of type of user in particular?  Does trust stem from some property of the decisions the system makes or from some other inherent characteristic of the machine?
A common approach to solving the difficulty in defining interpretability is to try and define it post-hoc by categorising systems into ontologies, based on their perceived interpretablility, but this seems like a circular way of approaching the problem: the classification of machine learning models is being done utilising the same criterion that is tried to be uncovered by doing so.
For reference, a commonly used classification is the following: 
\begin{itemize}
	\item \textbf{Opaque systems}: these are systems that offer no insight into the mapping between inputs and outputs; examples are all closed-source algorithms;
	\item \textbf{Interpretable systems}: this is the vastest category, as the characteristic of these systems is \textit{transparency} i.e. their inner workings are accessible but the onus of comprehensibility falls completely onto the user.  The classical example is that of neural networks where the mapping from inputs to outputs (the \textit{weights}) is inspectable by the user who can, theoretically and depending on her skill, interpret them;
	\item \textbf{Comprehensible systems}: systems falling into this category emit additional symbols together with their outputs with the explicit intent of giving the user the means to interpret and understand the automated decisions; the additional symbols may be visualisations, natural-language text or any other means of demystifying the output.  These extra symbols would need to be graded based on the user's expertise, as comprehension is a property that materialises on the human side.
\end{itemize}
Some authors propose to classify systems as \textit{non-interpretable}, \textit{ante-hoc interpretable/transparent} and \textit{post-hoc interpretable}; this roughly corresponds to the ontology presented above.

What I hope can be gleamed from this brief introduction to the field of Explainable Artificial Intelligence is that many of the problems it aims to tackle are hard \textit{per-se} and may not have a unique optimal solution.  This is because these issues are not only engineering problems but exist at the intersection between man and machine and as such can't be tackled using only the methods of Computer Science.  There is no way to satisfactorily investigate the human element of the situation without resorting to the well-established methods of the Social Sciences.  There is little hope to know in which way to procede without the guiding force that can only come from philosophy, because of its millennia-long tradition in thinking about ethical and high-level issues.  
It should be clear that when the human - and particularly the ethical - domain are part of the equation, it is impossible \textit{by definition} to find an optimal and unique solution.

 
\section{Problem and significance}
\textbf{identify the importance of the proposed research - how does it address the gap? state the research problem/ questions state the research aims and/or research objectives state the hypotheses
}

AI has a trust problem.  The bigger problem with AI is not anymore its utility, as that has mostly been solved by neural networks, but its capacity to elicit the trust of the users.
To be truly useful, an automated system should be able to make itself be trusted in a manner proportional to the criticality of its application.  Unfortunately, the explainability and, by extension, the ``trustability'' of machine learning models are inversely proportional.  There are many examples of modern methods - such as boosted trees, random forests, bagged trees, kernelized-SVMs - that show this tendency but this is best exemplified by deep neural networks, that are currently state of the art on a variety series of tasks but are among the least easily interpretable systems due to the fact that they represent information in an implicit and distributed manner among their network weights.  Some older methods, like decision trees or rule-based methods, are inherently more interpretable due to their simplicity and explicit reasoning steps, but are less accurate and flexible than modern techniques. 

The runaway success obtained by modern Machine Learning in a variety of domains, on a spectrum that goes from engineering to social work, has created the conditions to have a request to start applying these methods to mission-critical and traditionally more entrenched fields.  A perfect example of a field exhibiting both these characteristics is that of medicine.  The origins of AI date back to \textit{symbolic methods} integrated with \textit{knowledge-bases}, that were by design capable of providing an explanation for their reasoning and were thus accepted by the medical community and implemented into so-called \textit{expert systems}.  The deficiency of modern AI methods in being able to provide causal links for their reasoning process has held back their acceptance in the field of medicine, regardless of their superior performance and accuracy.

In a high-stakes domain such as the medical one, it would be unthinkable for a doctor to trust the predictions of an AI system a priori; any decision with profound moral implications - such as prescribing or interrupting a treatment of a patient - would have to first be validated by a human.  The possibility of carrying out this validation and its quality are dependent on the degree of interpretability of the model that made the decision.  Unfortunately, as has been repeated many times, the best performing models are often also the most opaque to inspection.

Explainability is not a necessary condition only for the verification of the system which, as we have just discussed, is a presupposition for the system to be applied in mission-critical domains but also for the extraction of knowledge.  The amount of information that a machine learning model can process is many orders of magnitude greater than that inspectable by any human; this may let a computer spot new patterns in the data that aren't immediately apparent or are latent given only a moderate amount of samples.  Being able to turn this information into new knowledge implies the system having the ability to output human-interpretable symbols that are capable of communicating it in a comprehensible and effective way.
The work of this thesis blablabla %TODO continuare a spiegare come quello che faccio si inserisce in questa vena
  
\section{Response}
\textbf{outline the methodology used - 
outline the order of information in the thesis - a roadmap - Maximum 2500 words.}

\chapter{Literature review}
What is explainability?
How is it defined?  By whom?  When?
Why is it important?
Notable works in the field

\chapter{Methodology}

\chapter{Results}

\chapter{Conclusions}


\chapter[Short title]{A chapter title which will run over two lines --- it's for
  testing purpose}

\lipsum[1-2]

\section{The first section}
\lipsum[3-4]

 \section{The second, math section}

\textbf{Theorem 1 (Residue Theorem).}
Let $f$ be analytic in the region $G$ except for the isolated singularities $a_1,a_2,\ldots,a_m$. If $\gamma$ is a closed rectifiable curve in $G$ which does not pass through any of the points $a_k$ and if $\gamma\approx 0$ in $G$ then
\[
\frac{1}{2\pi i}\int_\gamma f = \sum_{k=1}^m n(\gamma;a_k) \text{Res}(f;a_k).
\]
\textbf{Theorem 2 (Maximum Modulus).}
\emph{Let $G$ be a bounded open set in $\mathbb{C}$ and suppose that $f$ is a continuous function on $G^-$ which is analytic in $G$. Then}
\[
\max\{|f(z)|:z\in G^-\}=\max \{|f(z)|:z\in \partial G \}.
\]

\section[third]{A very very long section, titled ``The third section'', with
  a rather  short text alternative (third)}
\lipsum \texttt{Some Test}
\lstset{language=algebra,linewidth=0.95\linewidth,breaklines=true,numbers=left,
basicstyle=\ttfamily,numberstyle=\tiny,escapeinside={//*}{\^^M},
mathescape=true}
\begin{lstlisting}
import IntSpec, ItemSpec;

sort cart; //*\label{sort}

constructors //*\label{begin-sig}
create() $\longrightarrow$ cart;
insert(cart, item) $\longrightarrow$ cart;
observers
amount(cart) $\longrightarrow$ int;
transformers
delete(cart, item) $\longrightarrow$ cart; //*\label{end-sig}

axioms //*\label{begin-axioms}
forall c: cart, i, j: item 

amount(create()) $=$ 0; //*\label{begin-amount}
amount(insert(c,i)) $=$ amount(c) $+$ price(i); //*\label{end-amount}
delete(create(),i) $=$ create(); //*\label{begin-delete}
delete(insert(c,i),j) $=$
if (i =$\:$= j) c
else insert(delete(c,j),i); //*\label{end-axioms}
end
\end{lstlisting}

As you can easily see from the above listing \citet{bbggs:iet07}
define something weird based on the BPEL specification
\citep{bpelspec}.
\nocite{*}

\appendix %optional, use only if you have an appendix

\chapter{Some retarded material}
\section{It's over\dots}
\lipsum 

\backmatter

\chapter{Glossary} %optional

%\bibliographystyle{alpha}
%\bibliographystyle{dcu}
\bibliographystyle{plainnat}
\bibliography{biblio}

%\cleardoublepage
%\theindex %optional, use only if you have an index, must use
	  %\makeindex in the preamble
\lipsum

\end{document}
