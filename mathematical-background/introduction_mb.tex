\section{Introduction} \label{sec:mathematical-background-introduction}
This chapter will introduce and build up to a formal definition of Bayesian networks, a class of \textit{probabilistic graphical models} used to represent systems under conditions of uncertainty.

The chapter is organised as follows:
\begin{itemize}
  \item Section \ref{sec:probability-theory} introduces a series of basic concepts from probability theory focusing mainly on the basic concept of random variables and also establishing the notions of conditioning, independency and correlation.
  \item Section \ref{sec:information-theory} presents information entropy and uses it to define entropy measures for random variables, \textit{Kullback-Leibler divergence}, and distance measures for other objects, \textit{Hamming and Jaccard distances}.
  \item Section \ref{sec:graph-theory} introduces the objects of graphs and polytrees and the central concept of \textit{d-separation}.
  \item Section \ref{sec:bayesiannetworks} uses the content of the previous sections to define the Bayesian network formalism and then gives an overview of structure learning algorithms and the notions of \textit{conditional probability} and \textit{maximum a posteriori queries}.
\end{itemize}

Not all the concepts introduced in this chapter are strictly needed for the description of the Bayesian network formalism, but all will be useful as a mathematical reference for the methods developed in later chapters of this thesis.