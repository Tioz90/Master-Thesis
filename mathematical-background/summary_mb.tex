\section{Summary}
The chapter has introduced a number of concepts from probability, information and graph theory to be used as groundwork for the formal description of Bayesian networks, a widely used class of probabilistic graphical models.

The section dealing with probability theory opens by describing \textit{random variables}, a mathematical construct that associates a value to every outcome in the set of possible events, which is used to bring to the fore the attributes of interest while dealing with them in a clean way.
The probability of an event may be interpreted through the \textit{frequentist} or the \textit{Bayesian} lens; the former sees probability as simply the limit of the ratio between the number of times the event of interest occurred and the total number of trials; the latter views probabilities as the subjective degree of belief regarding the manifestation of the event.
The main results presented are \textit{Bayes' Theorem}, which states how to update a prior belief in the light of new knowledge, and the concept of \textit{independence} between events, which will be central when introducing \textit{d-separation}.

The second section introduces a few key concepts from information theory relating to entropy and distance measures.
\textit{Entropy} is a measure for the expected amount of information carried by a random variable, first introduced by Claude Shannon by drawing parallels to mechanical statistics. 
A convenient derived quantity is \textit{normalised entropy} - also known as \textit{efficiency}; this measure varies in $[0,1]$ and thus enables random variables and probability distributions of different cardinalities to be compared.
A second method, closely related to entropy, of measuring the interrelatedness of two random variables is that of \textit{mutual information}; this measure quantifies the amount of information of one variable already contained in the other.
Three popular distance measures are introduced: \textit{Kullback-Leibler divergence, Hamming and Jaccard distances}; the first is closely connected to the concept of entropy and is another measure for the interrelatedness between two variables, the second measures the similarity of strings, based on the number of substitutions needed to transform one into the other, and the last quantifies the similarity of sets, given the size of their intersection over union.

The third section relates to graph theory and starts by defining the basic notion of \textit{directed graph} and of \textit{directed acyclic graph}, a special case of graph presenting no cycles between vertices.
\textit{Trees} and \textit{polytrees} are briefly introduced and characterised, for future use.
Finally, \textit{d-separation}, first introduced by Judea Pearl, is discussed.
D-separation is a concept relating to the conditional dependence between variables; sets of variables may become independent i.e., not influence each other, based on conditioning on a third set of evidence variables.
The independence properties depend on the topology of the graph, specifically in how the variables of interest are connected to each other; they may be organised into \textit{chains}, \textit{forks} or \textit{colliders}.

The final section of the chapter deals with introducing \textit{Bayesian networks}, using many of the concepts laid out in the previous sections.
A Bayesian network is a probabilistic graphical model represented by a DAG where each vertex corresponds to a random variable and the edges model the dependencies among these.
The basic idea is to factorise a complete joint distribution of the constituent variables into a series of conditional probability distributions, one for each variable, that are assigned to the nodes in the DAG.
The defining characteristic is that each variable's node values depend only on those of its parents.
Such a representation efficiently represents a joint distribution and very naturally models the type of mixed causal and stochastic processes found in Nature.
The DAG of a BN can either be given or learned directly from data; learning is a super-exponential problem and there are three main classes of algorithms that may be applied to solve it: \textit{search and score}, \textit{constraint learning} and \textit{approximations}.
Once a DAG has been learned, the problem moves to querying (\textit{updating}) the BN; the main classes of queries are \textit{conditional probability} and \textit{maximum a posteriori queries}.
The first class asks for the value of a set of variables given the observation of the values of others in the network.
The second class, known as MAP queries, asks to find the most probable assignment of values to a subset of variables, given the observation of the values of another subset.
This is, in general, a hard problem but efficient solutions exist for a special case known as the \textit{most probable explanation}, where the set of query variables is the complementary subset to the evidence one.