\section{Discussion}
This thesis sets out as an investigation into the \textit{explainability} of Bayesian Networks.

The motivation leading to such a research question is to be found in the recent surge in the use of AI that is more and more integrated into the fabric of our society (see Section \ref{sec:intro-context}).
It has thus become imperative for these systems to be \textit{explainable}; that is, for its users to be able to understand the \textit{reasoning} behind the machine's outputs (see Section \ref{sec:explainability}).
This need is even more pressing in mission-critical domains such as that of Medicine.

The initial basis for the research carried out in this thesis has been the paper \citep{Butz2018} (see Section \ref{sec:explaining-the-most-probable-explanation}) that, while proposing a seemingly appealing method to enable the understanding of a medical data set, failed - as many other xAI works do - to provide any validation for its claims.
Thus a proof of concept system was developed\footnote{The complete source code is available at \url{https://github.com/Tioz90/Bayesian-Networks-Explainability-Tool}} in order to make an attempt at validating the claims made by the paper and, more in general, investigate the ability of Bayesian Networks to provide meaningful explanations to their users.
The benchmark against which the system's outputs have been compared are the \textit{explainability framework for BNs} offered by \citet{lacave2002review} and also the \textit{psychological characteristics inherent to an explanation} identified by \citet{miller2018explanation} (see Section  \ref{sec:explainability-in-bayesian-networks}).
One of the main gaps in the xAI literature has been the absence of real validation of the models being proposed by researchers, so among the main objectives of this thesis there was also to attempt to provide a methodological framework for the evaluation of machine learning systems with real domain experts i.e., an \textit{application-grounded evaluation} \citep{doshi2017towards} (see Section \ref{sec:evaluation-of-explainability}).

The prototype system was created by the implementation of standard techniques (see Subsection \ref{subsec:algorithms}) and the development of novel ones (see Subsection \ref{subsec:algorithms-novel}).
The underlying Bayesian Network was learned by using a real medical data set (see Section \ref{sec:data-set}).
This data set was provided by the medical partner, the Istituto Cantonale di Patologia in Locarno (Switzerland), who had a high degree of involvement at every step of this work (see Subsection \ref{subsec:istituto-cantonale}).
The expert pathologists at the ICP helped in both informing of the desiderata of the developed system but, most importantly, were crucial in validating it from a clinical relevance point of view and as regards its ability to interact meaningfully with them, that is they evaluated its \textit{explainability}.

The clinical relevance was evaluated by asking the ICP to define a series of natural language clinical questions that were then mapped onto the system's interaction modes (see Subsection \ref{subsec:clinical-validation-methodology} and \nameref{chap:annex}).
The results that the tool gave to these queries were compared by the experts at the ICP with those that they would have expected, based on medical literature and their personal expertise (see Subsection \ref{subsec:clinical-validation-results}).
In this respect, it has emerged that the tool, and the underlying BN, were able to capture and reply in a significant way to nearly all these questions thus validating it from a clinical relevance point of view.
This first validation step was important in order to establish a solid basis for the users to trust the system; if the system had not been able to implement the questions asked by the ICP or to offer them answers conforming to their expectations, it would have then been very difficult for it to then provide any meaningful explanation to the medical experts (see Section \ref{sec:importance-of-explainability}).
This because the users would not trust its outputs and, as discussed throughout Chapter \ref{chap:literature-review}, an explanation becomes such by virtue of a - sometimes metaphorical - dialogue between an \textit{explainer} (the machine) and an \textit{explainee} (the user).
If the two actors involved in an explanation are not able, or willing, to interface in a certain way, an explanation simply never comes into being.
The way - outputs, trust ... - in which humans and machines should interact in order for the former to explain something meaningful to the latter and the ways to elicit this interchange, is the focus of the Explainable AI field.

The evaluation of the explanatory powers of the tool was instead carried out by both an \textit{informal evaluation} (see Section \ref{sec:implemented-tool}), observing the experts using the tool and recording their impressions and issues, and a \textit{formal one} (see Subection \ref{subsec:explainability-validation} and \ref{subsec:explainability-validation-results}), involving an \enquote{Explainability Evaluation Questionnaire} (see \nameref{chap:annex}) geared towards probing the explainability of the system in the frameworks set by \citet{lacave2002review} and \citet{miller2018explanation}.
Both evaluations confirmed that the \textit{dialogical} explanation mode found in \citet{Butz2018} was the least effective means to offer the experts an explanation.
Thus, the claims made in the paper can not presently be substantiated by this thesis, but this result does obviously not disprove the explainability of Bayesian networks as a whole.
Another result, confirmed by both evaluations, was that the experts were very biased against preferring the \textit{verbal} explanation modality over any other; this seems to disprove the statement \enquote{the most direct and intuitive way of showing the information embodied in a Bayesian network is to display the corresponding graph} \citep{lacave2002review}.
If the explainability of Bayesian Networks is to be approximated in the satisfaction of the users of the system, then the work carried out in this thesis certainly seems to be a step in the direction of confirming the explanatory powers of such graphical models.
The experts of the ICP confirmed that they were able to interact meaningfully and fruitfully with the system and expressed the wish to continue using it on newer data sets.
The developed proof of concept system presented a mix of \textit{static} and \textit{dynamic} explanations together with \textit{contrastive}, \textit{linguistic} and \textit{visual} output modalities.
Even though the \enquote{dialogues}, that are instances of a \textit{dynamic}, \textit{contrastive}, \textit{linguistic} and \textit{visual} explanation, were not easy for the users to use meaningfully, the other explanatory modes like \enquote{MPE}, \enquote{pseudo-MPE} and \enquote{conditional probability queries} seemed to completely satisfy the experts at the ICP.
These other explanatory modes make use of one of the characteristics of BNs, namely that their outputs are \textit{selected}.
The results in this thesis thus seem to indicate that simple \textit{selection} of the outputs may be more important to medical users that the other characteristics - present in the \enquote{dialogues} and to a certain extent in \enquote{pseudo-MPE queries} - of explanations identified by \citet{miller2018explanation}: \textit{contrastiveness}, \textit{causality} and \textit{sociality}.

As regards laying out an \textit{evaluation methodology groundwork} for future \textit{application-based evaluations} of machine learning models in the medical domain, the hope is certainly to have done so fruitfully, barring the limitations recognised in Section \ref{sec:future-work}.
The evaluation methodology was able to surface results that were not in line with the established literature and thus these should surely merit further consideration.
Time was also included as part of this evaluation and this presents quite an element of novelty because, as noted by \citet{gilpin2018explaining}, this element is usually missing in xAI literature.
Nonetheless, many interesting results specific to the medical domain (reported throughout Chapter \ref{chap:results} where relevant) were surfaced through the application of the \textit{research methodology} and the resulting prototype system was also warmly received by its expert users.
Thus, one could certainly feel a certain degree of confidence in this research methodology being able to accurately characterise the domain of interest and to inform the building of effective \textit{explanatory tools} within it.