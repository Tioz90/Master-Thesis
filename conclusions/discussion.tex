\section{Discussion}
The purpose of this thesis is to investigate the \textit{explanatory powers} of Bayesian networks.

The motivation for undertaking this research is connected to the recent surge in the use and integration of AI into the fabric of our societies (see Section \ref{sec:intro-context}).
It has thus become imperative for these systems to be \textit{explainable}; that is, for its users to be able to understand the \textit{reasoning} behind the machine's outputs (see Section \ref{sec:explainability}).
This need is even more pressing in mission-critical domains such as that of \textit{medicine}.

The basis for the research developed in this thesis was the paper \enquote{Explaining the Most Probable Explanation} \citep{Butz2018} (see Section \ref{sec:explaining-the-most-probable-explanation}).
This foundational work, while proposing a seemingly appealing method to enable the understanding of a medical data set, failed - as many other xAI works do - to provide any validation for its claims.
Thus a proof of concept system was developed\footnote{\url{https://github.com/Tioz90/Bayesian-Networks-Explainability-Tool}} with the purpose of validating the claims made by the paper and, more in general, to investigate the ability of Bayesian networks to provide meaningful explanations to their users.
The benchmark against which the system's outputs have been compared have been the \textit{explainability framework for BNs} offered by \citet{lacave2002review} and also the \textit{psychological characteristics inherent to an explanation} identified by \citet{miller2018explanation} (see Section  \ref{sec:explainability-in-bayesian-networks}).
One of the main gaps in the xAI literature has been the absence of substantial validation of the models being proposed by researchers; therefore one of the main objectives of this thesis was to provide a methodological framework for the evaluation of machine learning systems with real domain experts i.e., an \textit{application-grounded evaluation} methodology \citep{doshi2017towards} (see Section \ref{sec:evaluation-of-explainability}).

The prototype system was created by the implementation of standard techniques (see Subsection \ref{subsec:algorithms}) and the development of novel ones (see Subsection \ref{subsec:algorithms-novel}).
The underlying Bayesian network was learned through use of a real medical data set (see Section \ref{sec:data-set}), which was provided by a medical partner with a high degree of involvement at every step of this research process (see Subsection \ref{subsec:istituto-cantonale}).
Expert pathologists aided by informing of the desiderata of the developed system and particularly in the crucial stage of validating it from a clinical relevance point of view and as regards its ability to interact meaningfully with them.
That is, they both informed the design and evaluated its \textit{explainability}.

The clinical relevance was evaluated by asking the medical experts to define a series of natural language clinical questions, which were then mapped and executed on the system's user interaction modes (see Subsection \ref{subsec:clinical-validation-methodology} and Appendix \ref{app:natural-language-questions}).
The ensuing results from these queries were compared by the medical experts with those that they would have expected, based on medical literature and their personal expertise (see Subsection \ref{subsec:clinical-validation-results}).
In this respect, it has been shown that the tool, and the underlying BN, were able to capture and respond in a significant manner to nearly all these questions thus validating the software and ML model from a clinical relevance point of view.
This first validation step was important in order to establish a solid basis for the users to trust the system; if the system had been incapable of implementing the questions asked by the users or of offering them answers conforming to their expectations, it would have then been very difficult for it to then provide any meaningful explanation to the medical experts (see Section \ref{sec:importance-of-explainability}).
This is because the users would not have trusted its outputs and, as discussed throughout Chapter \ref{chap:literature-review}, an explanation becomes such by virtue of a dialogue between an \textit{explainer} (the machine) and an \textit{explainee} (the user).
If the two actors involved in an explanation are not able, or willing, to interface in a certain way, an explanation simply never comes into being.
The manner in which humans and machines should interact (e.g., in terms of outputs, trust) in order for the former to explain something meaningful to the latter and the ways to elicit this interchange, are the focus of the explainable AI field.

The evaluation of the explanatory powers of the tool was carried out by both an \textit{informal evaluation} (see Section \ref{sec:implemented-tool}), consisting in observing the experts using the tool and recording their impressions and issues, and a \textit{formal one} (see Subsection \ref{subsec:explainability-validation} and \ref{subsec:explainability-validation-results}), involving an \enquote{explainability evaluation questionnaire} (see Appendix \ref{app:questionnaire}) geared towards probing the explainability of the system compared to the concepts given by \citet{lacave2002review} and \citet{miller2018explanation}.
Both evaluations confirmed that the \textit{dialogical} explanation mode proposed in \citet{Butz2018} was the least effective means to offer the experts an explanation.
Thus, the claims made in the paper cannot presently be substantiated by this thesis; however, such results do not disprove the explainability of Bayesian networks as a whole.
Another result, confirmed by both evaluations, was that the experts were very biased towards preferring the \textit{linguistic} explanation modality over any other; this seems to disprove the statement \enquote{the most direct and intuitive way of showing the information embodied in a Bayesian network is to display the corresponding graph} \citep{lacave2002review}.
But, this result is also a step in vindicating the initial claim made in \enquote{Explaining the Most Probable Explanation} \citep{Butz2018} that BNs are hard to interpret for medical domain experts, even though they provide a graphical representation of their knowledge base.
These authors attempted to solve this issue through the use of \textit{dialogue} but, as discussed, the conclusions of this thesis can only confirm that this is still an open problem.
If the explainability of Bayesian networks is to be approximated in the satisfaction of the users of the system, then the work carried out in this thesis certainly seems to be a step in the direction of confirming the explanatory powers of such graphical models.
The medical experts confirmed that they were able to interact meaningfully with the system and expressed the desire to continue using it on newer data sets.

The developed proof of concept software tool presented a mix of \textit{static} and \textit{dynamic} explanations together with \textit{contrastive}, \textit{linguistic} and \textit{graphical} output modalities.
Even though the \enquote{dialogues} - which are instances of a \textit{dynamic}, \textit{contrastive}, \textit{linguistic}, and \textit{graphical} explanation - were not easy for the users to use meaningfully, the other explanatory modes - consisting of \enquote{MPE}, \enquote{pseudo-MPE}, and \enquote{conditional probability} queries - seemed to completely satisfy the experts.
These other explanatory modes are notable examples of the kind of \textit{uncertain reasoning} that can be performed with Bayesian network inference, provided that suitable algorithms for those computational tasks are provided.
They also make good use of one of the core characteristics of BNs, namely that their outputs can be \textit{selected}.
This selectivity means that an explanation may contain only the information necessary to be efficiently conveyed to the user; neural networks, for example, are not capable of such an output.
The results of this thesis thus seem to indicate that simple \textit{selection} of the outputs may be more important to medical users that the other characteristics - present in the \enquote{dialogues} and to a certain extent in \enquote{pseudo-MPE queries} - of explanations identified by \citet{miller2018explanation}: \textit{contrastiveness}, \textit{causality} and \textit{sociality}.

Regarding the objective of laying out an \textit{evaluation methodology groundwork} for future \textit{application-based evaluations} of machine learning models in the medical domain, we believe that the work achieved by this thesis - barring the limitations recognised in Section \ref{sec:future-work} - was a worthwhile undertaking.
The evaluation methodology served to develop results which did not align with the established literature and these thus merit further consideration; for example, the fact that the medical experts preferred \textit{linguistic} explanations over any other.
The \textit{time} needed to understand an explanation was also included as part of this evaluation and this presents an element of novelty because, as noted by \citet{gilpin2018explaining}, the temporal component is usually disregarded in xAI literature.
Nonetheless, many interesting results specific to the medical domain (reported throughout Chapter \ref{chap:results} where relevant) were brought to light through the application of the \textit{research methodology} and the prototype system, which was also a result of it, was warmly received by its expert users.
This supports the soundness of this research methodology in its capacity to accurately characterise the domain of interest and to inform the building of effective \textit{explanatory tools} within it.