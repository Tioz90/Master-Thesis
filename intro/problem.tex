% !TEX root = thesis-thomas-tiotto.tex

\section{Problem and Significance}
\textbf{identify the importance of the proposed research - how does it address the gap? state the research problem/ questions state the research aims and/or research objectives state the hypotheses
}

\todo[inline]{again, references!}

AI has a trust problem.  The bigger problem with AI is not anymore its utility, as that has mostly been solved by \todo{ok but come on, there is not only NN in this world} deep neural networks, but its capacity to elicit the trust of the users.
To be truly useful, an automated system should be able to make itself be trusted in a manner proportional to the criticality of its application.  Unfortunately, the explainability and, by extension, the \enquote{trustability} of machine learning models are inversely proportional.  There are many examples of modern methods - such as boosted trees, random forests, bagged trees, kernelized-SVMs - that show this tendency, but it is best exemplified by\textit{ deep neural networks} (DNN). Deep Neural Networks are machine learning models constructed by stacking many layers of artificial neurons, these systems are currently state of the art on a variety of tasks but are among the least easily interpretable systems due to the fact that they represent information in an implicit and distributed manner among their network weights.  Some older methods, like decision trees or rule-based methods, are inherently more interpretable due to their simplicity and the fact that they can explicit state their reasoning steps, but are less accurate and flexible than more modern techniques. 

The runaway success obtained by modern Machine Learning in a variety of domains, on a spectrum that goes from engineering to social work, has created the desire to also start applying these methods to mission-critical and traditionally more entrenched fields.  A perfect example of a field exhibiting both these characteristics is that of medicine.  The first successful artificially intelligent systems date back to the 1970s and '80s and were based on \textit{symbolic methods} integrated with \textit{knowledge-bases}.  These systems were by design capable of providing an explanation for their reasoning and were thus accepted by the medical community in an implementation known as \textit{expert systems} that aimed to perform functions similar to those of a human expert.  The deficiency of modern AI methods in being able to provide causal links for their reasoning process has held back their acceptance in the field of medicine, regardless of their superior performance and accuracy.

In a high-stakes domain such as the medical one, it would be unthinkable for a doctor to trust the predictions of an AI system a priori; any decision with profound moral implications - such as prescribing or interrupting the treatment of a patient - would have to first be validated by a human.  The possibility of carrying out this validation and its quality are dependent on the degree of interpretability of the model that made the decision.  Unfortunately, as has been repeated many times, the best performing models are often also the most opaque to inspection.

Explainability is not a necessary condition only for the verification of the system which, \todo{not clear. you were speaking about "validation" before. is this taken as a synonymous of "verification"? if yes, why two words for the same concept? if not, then explain better} as we have just discussed, is a presupposition for it to be applied in mission-critical domains, but also \todo{why?} for the extraction of knowledge from data.  The amount of information that a machine learning model can process is many orders of magnitude greater than that inspectable by any human; this may let a computer spot new patterns in the data that aren't immediately apparent or are latent given only a moderate amount of samples.  Being able to turn this information into \todo{so, what is knowledge? why explainability is necessary to create knowledge? you do not explain this here, imho} new knowledge implies the system having the ability to output human-interpretable symbols that are capable of communicating it in a comprehensible and effective way.

There has recently been much research carried out on trying to explain and extract knowledge from deep neural networks together with attempts to marry the connectionist and symbolic approaches to artificial intelligence - a subfield known as \textit{neuro-symbolic computation} while also reconsidering mixed approaches such as \textit{Bayesian Networks}.
A Bayesian Network is a graphical and computationally efficient way of representing dependencies between random variables.  The graphical component is immediate as in the model each random variable is represented by a node of a Directed Acyclic Graph (DAG), with the edges connecting them standing for their dependencies.  The efficiency stems from the fact that the graph structure imposes a factorisation of the joint probability space and thus lets each variable be calculated using only the values of its parents.

\todo[inline]{again, a lot of nice talk about NN, but then little about BN and thus what you are gonna do}