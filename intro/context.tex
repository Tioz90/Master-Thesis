% !TEX root = thesis-thomas-tiotto.tex

\section{Context}
\textbf{state the general topic and give background of what your reader needs to know to understand the problem outline the current situation evaluate the current situation (advantages/ disadvantages) and identify the gap}

\todo[inline]{In genere mancano referenze}

While neural networks and Artificial Intelligence (AI) - as a field - have existed for nearly seventy years, the concept of artificial intelligence dates back to at least Ancient Greece.  In ancient times, artificial intelligence embodied in mechanical men was part of the domain of myth; in the twentieth century, of that of science.  During this last decade, Artificial Intelligence can't anymore \todo{What do you mean? Why ``reductive''?} be described by a limited set of terms, as it has materialised out of Man's imagination, broken out of laboratories and has been given lease to act in the world at large.


No sector of our economy has been left untouched by the recent and rapid rise of machine learning that has been enabled by the rediscovery of deep neural networks, the availability of Big Data and cheap parallel computing power.  Fields as diverse and as critical as are government, healthcare, finance and bioinformatics have been revolutionised and the possibility has been set for new ones - such as self-driving vehicles - to be born.  
The ever increasing reliance of our society on ever more complex machine learning-driven algorithms can only make us worry ever more about the ethical dilemmas posed by such a situation.  
Our society has only very recently been confronted with the dilemma of assigning blame when a driverless car causes the death of a person but this moral problem is only the tip of the iceberg, even when focusing only on the automotive industry.  For example, how should a self-driving car behave when confronted with a real-world analogous of the classic Trolley Problem - a situation where each course of action is liable to cause harm?  
On what basis should a person be denied a mortgage, access to university or a job interview?  How can we be sure that there is no bias in the system?  How do we even define if the system is behaving morally?  Would it currently be feasible for a person that feels they have been harmed by such a decision to appeal it, as prescribed by the recent EU General Data Protection Regulation (GDPR)? 
As more and more decisions are made in an automated way, with many of them significantly impacting both individuals and society at large, it comes natural to stop and wonder what are the characteristics we would want the systems making these decisions to have.   

\textit{Explainable AI }(xAI) is the sub-field of AI that rests at the intersection between Computer Science, Social Sciences and Philosophy and whose aim is to define our desiderata of artificially intelligent systems and machine learning algorithms from the point of view of their explainability.  The basic idea is that the prerequisite for the evaluation of the ethical and moral implications of a machine's decision is for the system to be \enquote{interpretable} or \enquote{explainable}.  
Within the xAI community, \todo{references to articles ?} there is currently no unanimously agreed upon definition of which these desiderata should be or of the best way to implement them in real systems.
There is also no common, agreed-upon, definition of what is meant by the phrase \enquote{understanding a system}: some authors equate \todo{explain better what follows here, since it is important} it to having a \todo{what does functional mean?} \textit{functional understanding}, void of the \todo{what are low level details} low-level details, while others decline it into the  concepts of \textit{interpretation} and \textit{explanation}, the former indicating \todo{??? explain better. gives examples, help the reader} the output of a format that a human user can comprehend and the latter a set of features that have contributed to generating the system's decision. 

The difficulties start even in trying to define what interpretability really is.  \todo{does not sound well expressed here. also what has trust to do with interpretability? explain, give intuitions, motivations} Does it mean to gain the trust the system's user?  Of type of user in particular?  Does trust stem from some property of the decisions the system makes or from some other inherent characteristic of the machine?
A common approach to solving the difficulty in defining interpretability is to try and define it post-hoc by categorising systems into \todo{why are you speaking about ontologies here (and not simply of categories etc?} ontologies, based on their perceived interpretablility; unfortunately this seems like a circular way of approaching the problem: the classification of system models is being done utilising the same criterion that is trying to be uncovered by doing so.
For reference, a commonly used classification is the following: 
\begin{itemize}
	\item \textbf{Opaque systems}: these are systems that offer no insight into the mapping between \todo{in general. explain what a system, input and output are} inputs and outputs; all closed-source algorithms fall under this definition;
	\item \textbf{Interpretable systems}: this is the vastest category, as the characteristic of these systems is \textit{transparency} i.e. their inner workings are accessible but the onus of comprehensibility falls completely onto the user.  The classical example is that of neural networks where the mapping from inputs to outputs (the \textit{weights}) is inspectable by the user who can, theoretically and depending on her skill, interpret them;
	\item \textbf{Comprehensible systems}: systems falling into this category emit additional symbols together with their outputs with the explicit intent of giving the user the means to interpret and understand the automated decisions; the additional symbols may be visualisations, natural-language text or any other means of demystifying the output.  These extra symbols would need to be graded based on the user's expertise, as comprehension is a property that involves both man and machine but materialises on the human side.
\end{itemize}
\todo{References!!!} Some authors propose to classify systems as \textit{non-interpretable}, \textit{ante-hoc interpretable/transparent} and \textit{post-hoc interpretable}; this roughly corresponds to the ontology presented above.

What I hope can be gleamed from this brief introduction to the field of Explainable Artificial Intelligence, is that many of the problems it aims to tackle are hard \textit{per-se} and may not have a unique optimal solution.  This is because these issues are not only engineering problems, but exist at the intersection between man and machine and as such can't be tackled using only the methods of Computer Science.  There is no way to satisfactorily investigate the human element of the situation without resorting to the \todo{for instance? which methods? example?} well-established methods of the Social Sciences.  There is little hope to know in which direction to procede without the guiding force that can only come from philosophy, because of its millennia-long tradition in thinking about ethical and high-level issues.  
It should be clear that when the human - and particularly the ethical - domain are part of the equation, it is impossible \textit{by definition} to find an optimal and unique solution.

\todo[inline]{you have  to link what you just said with what you are gonna do later. for instance, illustrate what you are saying with examples. use one close to what you are going to do. also, you speak about NN but never about graphical methods, such as BN. you have to speak about them here too. }
