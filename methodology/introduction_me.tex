\section{Introduction} \label{sec:methodology-introduction}
 The inspiration for the work carried out in this thesis was the paper \citep{Butz2018} reviewed in detail in Section \ref{sec:explaining-the-most-probable-explanation}.
 This work proposed a system that would learn a \enquote{knowledge base} i.e., a tree representing the chain of most probable deductions, starting from a Bayesian network modelling a medical data set, rooted in a set of initial evidence.
 This tree, deemed to represent the solution to the MPE query (Definition \ref{def:mpe}), could then be used to generate a dialogue in natural language with the medical expert, which the authors claimed could lead to the extraction of extra knowledge from the original data set.  
 The driving hypothesis of the paper was that Bayesian networks and the solution to the MPE problem would be a powerful tool in helping medical experts gain insights into data.
 
The paper did not provide any indication that a such a system had ever been built and any validation of the method was left by the authors for future work.
As discussed in Chapter \ref{chap:introduction} and \ref{chap:literature-review}, this lack of real-world validation has been seen to be the unfortunate norm in most papers published under the explainable AI moniker.
 Many works are content to only give a \textit{functionally-grounded evaluation} (in the taxonomy of \citet{doshi2017towards}) for the methods they propose.
The one by \citet{Butz2018} does not even present such an evaluation.
 As of the finalisation of this thesis in early September 2019, there has been no work carried out in substantiating the methods of \citet{Butz2018}.
 As introduced in Chapter \ref{chap:introduction} and discussed in detail in Section \ref{sec:importance-of-explainability}, there is an ever greater need for machine learning models and systems to be explainable, especially in mission-critical domains such as healthcare.
 
 For these reasons the hope was that building a proof of concept system, whose logic was inspired by the method presented in the aforementioned paper, and validating it with real medical experts, could prove to be a positive endeavour.
 What is being proposed is to carry out an \textit{application-grounded evaluation} of the developed system.
 The objective of this thesis is not only to provide an assessment of the paper and of Bayesian networks in general, but also to set a methodological precedent for the evaluation of a machine learning system with real medical experts on an actual tasks.
 This, as discussed in Chapter \ref{chap:introduction} and \ref{chap:literature-review}, is one of the main gaps existing today in the field of xAI.
 Thus, carrying out such an evaluation could be seen as introducing an element of novelty.
 It is also hoped that the proof of concept system may be of concrete use to the medical experts who are provided with it, in performing their daily work.

 As already set out in Section \ref{sec:response}, the work carried out in this thesis benefited from a high degree of collaboration with a third party, the \textit{Istituto Cantonale di Patologia}, based in Locarno (Switzerland).

 The chapter is organised as follows:
 \begin{itemize}
  	\item Section \ref{sec:data-set} opens with an introduction to the medical partner involved in the evaluation of the methods: \textit{Istituto Cantonale di Patologia}. The institute is based in Locarno in the Swiss canton of Ticino and specialises in the histological analysis of tissue samples in support of cancer diagnosis.
 	\item Section \ref{sec:methods} gives an overview of the standard tools used in building the proof of concept system, which was then evaluated with the pathologists of the ICP.
% 	and includes a brief survey of classic machine learning methods, with the objective of making the reader familiar with them before describing how they are used as a benchmark for the BN's classification performance.
 	\begin{itemize}
  		\item Subsection \ref{subsec:libraries} presents the main Python libraries employed in implementing the system.
  		\item Subsection \ref{subsec:algorithms} shows the algorithms that are part of the methods of this thesis but that make use of standard techniques; these include a classic algorithm for \textit{d-separation} by \citet{koller2007}.	
	\end{itemize}
	\item Section \ref{sec:novel-contributions} introduces the novel algorithms and methods that were developed specifically for the implementation of the prototype system and explains the rationale behind the selection method used to build the \enquote{knowledge base} (see Section \ref{sec:explaining-the-most-probable-explanation}) and presents an algorithm for the calculation of the \textit{mutual information} (see Subsection \ref{subsec:mutual-information}) between pairs of variables in the BN.
	\begin{itemize}
  		\item Subsection \ref{subsec:algorithms-novel} gives a detailed presentation of the methods developed, including the three variants of the \textit{dialogue} - which are adaptations and developments to the method presented by \citet{Butz2018}, an algorithm to generate alternative explanation branches when the domain expert disagrees with the system during a dialogue, a greedy algorithm to construct a \enquote{pseudo-MPE} branch from random evidence and a procedure to compare it to the true MPE solution.
  		\item Subsection \ref{subsec:interfacing-user} concentrates on the methods needed to interface effectively with the user.
	\end{itemize}
	\item Section \ref{sec:validation} presents the methodology used to validate the proof of concept tool, which implements all of the methods presented in the previous sections, from the point of view of its clinical relevance and its capacity to surface explainations to the user.
\end{itemize}