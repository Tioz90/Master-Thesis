\section{Validation Methodology} \label{sec:validation}
Having direct access to expert pathologists has not only helped in guiding research into the theoretical explainability properties of the system but also enabled their \textit{application-grounded evaluation} (see Section \ref{sec:evaluation-of-explainability}).
There are two main validation points of view to be addressed: the clinical (Subsection \ref{subsec:clinical-validation-methodology}) and the explainability (Subsection \ref{subsec:explainability-validation}), with the results of the latter depending on part on those of the former.
 
\subsection{Clinical Validation} \label{subsec:clinical-validation-methodology}
A validation of the methods carried out in this thesis in their adherence to established clinical literature is of paramount importance.
A failure on the Bayesian network's part in capturing the true relationships between the variables would hamper it in being able to give any meaningful representation of them.
For the experts to even start to trust the system or to be able to make sense of its outputs, it is vital that there be as little cognitive dissonance between their basic beliefs and expectations and those that they see represented in the system.

For this reason, the initial validation phase with the ICP concentrated on the clinical aspect.
The methodology chosen to clinically validate the system was for the ICP to formulate a series of natural language queries; each one of these questions was annotated with the queried variable and its value, together with the values of any evidence variables.
The experts included the expected reply to the queries together with its likelihood, based on the latest medical literature and their personal, knowledge-based expertise.
These questions can be abstracted as:
\begin{center}
\enquote{Given that the value of $var_1$ is $a_1$ and $\ldots$ and the value of $var_n$ is $a_n$, what is the probability that $var_{n+1}$ takes value $a_{n+1}$?}.	
\end{center}

The natural language questions formulated by the ICP can be classified along two axes:
\begin{itemize}
  \item based on their intended purpose: \textit{validation} vs. \textit{research}.
  The former questions' replies are known from established clinical literature and are the queries that will actually be used to validate the system from a clinical point of view.
  The latter are queries that don't have a definite clinical answer but that are nonetheless extremely interesting in helping to understand the types of questions a domain expert may want to ask the system.
  \item based on the way they may be answered: by a \textit{conditional probability query} (Definition \ref{def:conditional-probability}), a \textit{d-separation query} (Definition \ref{def:d-separation}) or an \textit{MPE query} (Definition \ref{def:mpe}).
\end{itemize}
The complete series of thirty questions has been organised according to the second criterion.
Appendixes \ref{app:conditionalprobability1} and \ref{app:conditionalprobability2} present fourteen questions that can be answered by conditional probability queries.
Appendix \ref{app:dseparation} shows a series of eight natural language questions that can be answered by running a d-separation query.
Appendix \ref{app:conditionalanddseparation} presents five questions that could be answered by a conditional probability query but also, at a higher level, by a d-separation query.
This is because what is being asked, is basically wether changing the value of the evidence variable has an influence on that of the target variable.
This could be answered by running multiple conditional probability queries and comparing the resulting target variable values or, more simply, by checking if the target and evidence variables are d-connected or not.
The first method would give a finer grained answer as it would also \textit{quantify} the magnitude of the effect of one variable on the other; checking for d-separation would only give a \textit{qualitative} answer, which may nonetheless be sufficient. 
Finally, Appendix \ref{app:mpe} shows three questions that are naturally mapped onto a query of the MPE type.

Most importantly at this stage, all questions can be implemented on the proof of concept system and consequently this shows a good coverage on the tool's part of the use cases that can be imagined by a domain expert.
If the system can, in principle, answer every question imagined by the expert then this is an indication that it conforms to her \textit{worldview} and thus could be well positioned to interact fruitfully with her.

The questions marked as \textit{validation} will be posed to the system, in autonomy, by the ICP's representatives, who will then compare the outputs with the result they would have expected, based on established medical literature and their expertise.
The columns containing the experts' expected results and their comments have been omitted from the natural language questions shown in Appendix \ref{app:natural-language-questions} and included directly in the discussion of the results in Subsection \ref{subsec:clinical-validation-results}, alongside the system's outputs.
If the system's outputs conform to the experts' preconceived ideas in a high number of cases (as confirmed by the experts themselves) then the system can be said to have been \textit{clinically validated}.
This is important because the enabling condition for the user to trust the predictions made by the software is that these shouldn't be in strong discordance with her existing beliefs.
Not having a strong \textit{cognitive dissonance} is a \textit{necessary} - but not sufficient - condition to enable trust and therefore explainability.

\subsection{Explainability Validation} \label{subsec:explainability-validation}
In general, there is strong resistance to novelty in the field of medicine, both for ethical reasons and because of the need for clinicians to be conservative in attending to established best practices in the field.
Any tool that is too onerous in terms of time and cognitive load is liable to remain underutilised.
In this field, \textit{a tool must therefore only be the means by which a question is answered}, not itself become a question; the methods developed in this thesis aim to conform to this objective, barring the experimental nature of the software and the consequent lack of refinement of its interface.
The need for a comprehensible and efficient tool is especially present because the goal of a pathologist is to arrive at a diagnosis, containing the elements useful to define prognosis and therapeutical approach, in the briefest time possible.
The main reasons are ethical, since for a patient waiting for a report is extenuating, and clinical, because a timely diagnosis is the first factor at the base of life expectancy.
Obviously, the highest possible accuracy is always strived for.
The clinical field and that of biomedicine are forced to embrace uncertainty, as this is an integral part of their practice.
Consequently, any tool able to support in comprehension and decision-making is automatically useful, once it has been clinically validated; in other words, even though a specific system may not be decisive or applicable to all reviewed cases, it will nonetheless be taken into account.

Thus, a system validated in terms of its adherence to clinical literature could then also meaningfully be validated from an explainability point of view.
The main question to be addressed is its capacity to relate to the expert user.
Is the system able to engender the user's trust?
In doing so, is she able to extract more knowledge from existing data when using the system than not?
Especially in cases where there may be a dearth of data, can the expert maximise the benefit from the available information?
Does the user subjectively feel that the system may positively impact her work?
These are all hard questions to answer, as there is a very high degree of subjectivity involved.
Thus to attempt to answer them, the chosen methods were borrowed from the social sciences.

In an earlier stage, the experts were introduced to the system in prototype form and instructed on the use cases it offered.
This process would enable the collection of feedback on the functionalities of the system and help in shaping its subsequent design.

The finalised system was, in a later phase (early August 2019), provided to the experts at the ICP for use in their daily work.
To quantify the performance of the system, as perceived by its users in a real setting over an extended period of time, a follow-up was done after three weeks by way of an \enquote{explainability evaluation questionnaire}, designed to test the gaps identified in Chapter \ref{chap:literature-review}.
The full questionnaire can be found in Appendix \ref{app:questionnaire}.

The \enquote{explainability evaluation questionnaire} presents five sections:
\begin{itemize}
  \item \textit{confidence}: aimed at assessing wether the use of the system incremented the confidence the clinician felt in making her decisions;
  \item \textit{features}: to understand in more detail which interaction modes were perceived as most useful and the subjective reasons for this.
  Of particular interest is the understanding of the perceived quality of the dialogical interaction modes and of the \enquote{pseudo-MPE} query;
  \item \textit{time}: questions focusing on the the temporal element, mainly the time needed to understand various explanations offered by the system.
  This element is often overlooked in the relevant xAI literature (see Section \ref{sec:explainability-in-bayesian-networks});
  \item \textit{tool}: general questions regarding the use of tool and if any important use-case was felt to be missing;
  \item \textit{clinical}: investigating if the tool was clinically relevant in day-to-day work.
  Unlike the clinical validation presented in Subsection \ref{subsec:clinical-validation-methodology}, these questions investigate \textit{a posteriori} the use of the tool and as such should provide a broader evaluation of its clinical relevance;
  \item \textit{satisfaction}: simple question asking to rate the general satisfaction with the proof of concept system.
\end{itemize}

As discussed throughout Chapter \ref{chap:literature-review} and summarised in Section \ref{sec:literature-review-summary}, one of the main gaps in the field of explainable AI is the absence of real-world validation of the - supposedly - explainable models.
The objective of the questionnaire is to act as an \textit{application-grounded evaluation}, in the taxonomy proposed by \citet{doshi2017towards} and presented in Section \ref{sec:evaluation-of-explainability}, and thus provide what is considered the gold standard for the evaluation of a machine learning system.
Also included, since it is almost always neglected in literature, is a focus on the \textit{temporal element} of the explanations that was noted as important by \citet{gilpin2018explaining}.
Of particular interest is evaluating the Bayesian network - underlying the tool's capabilities - in its capacity to surface cogent explanations for the target user; the questionnaire inflects the questions in order to identify which particular characteristics of the system and BN were perceived by the user as the most useful in order to gain an understanding of the underlying data set.
As noted in Section \ref{sec:explainability-in-bayesian-networks}, by acknowledging the psychological characteristics of an explanation identified by \citet{miller2018explanation}, explanations have various essential characteristics that seem to also be inherent in BNs; the questionnaire thus seeks to understand if these are actually present and perceived as useful, in the sense of enabling explainability, by the domain experts.

The questionnaire is not the only source of the results relating to the \textit{application-grounded evaluation} of the developed system; similarly to \citep{stumpf2009interacting} in their \enquote{think-aloud experiment}, many results and details throughout Chapter \ref{chap:results} will be the outcome of observing and listening to the expert users while they were engaging with the system.
We refer to these as \enquote{informal explainability evaluation results} contrasting them with the \enquote{formal explainability evaluation results} that will be the outcomes of the questionnaire.